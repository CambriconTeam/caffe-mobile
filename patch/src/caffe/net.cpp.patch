--- caffe/src/caffe/net.cpp	2017-04-19 19:08:25.000000000 +0800
+++ ../src/caffe/net.cpp	2017-04-19 17:09:45.000000000 +0800
@@ -5,19 +5,27 @@
 #include <utility>
 #include <vector>
 
+#ifdef USE_HDF5
 #include "hdf5.h"
+#endif
 
 #include "caffe/common.hpp"
 #include "caffe/layer.hpp"
 #include "caffe/net.hpp"
+#ifdef NO_CAFFE_MOBILE
 #include "caffe/parallel.hpp"
+#endif
 #include "caffe/proto/caffe.pb.h"
+#ifdef USE_HDF5
 #include "caffe/util/hdf5.hpp"
+#endif
 #include "caffe/util/insert_splits.hpp"
 #include "caffe/util/math_functions.hpp"
 #include "caffe/util/upgrade_proto.hpp"
 
+#ifdef NO_CAFFE_MOBILE
 #include "caffe/test/test_caffe_main.hpp"
+#endif
 
 namespace caffe {
 
@@ -556,8 +564,10 @@
 template <typename Dtype>
 const vector<Blob<Dtype>*>& Net<Dtype>::Forward(
     const vector<Blob<Dtype>*> & bottom, Dtype* loss) {
+#ifdef USE_GLOG
   LOG_EVERY_N(WARNING, 1000) << "DEPRECATED: Forward(bottom, loss) "
       << "will be removed in a future version. Use Forward(loss).";
+#endif
   // Copy bottom to net bottoms
   for (int i = 0; i < bottom.size(); ++i) {
     net_input_blobs_[i]->CopyFrom(*bottom[i]);
@@ -565,6 +575,7 @@
   return Forward(loss);
 }
 
+#ifdef ENABLE_BACKWARD
 template <typename Dtype>
 void Net<Dtype>::BackwardFromTo(int start, int end) {
   CHECK_GE(end, 0);
@@ -583,6 +594,7 @@
     }
   }
 }
+#endif
 
 template <typename Dtype>
 void Net<Dtype>::ForwardDebugInfo(const int layer_id) {
@@ -610,6 +622,7 @@
   }
 }
 
+#ifdef ENABLE_BACKWARD
 template <typename Dtype>
 void Net<Dtype>::BackwardDebugInfo(const int layer_id) {
   const vector<Blob<Dtype>*>& bottom_vec = bottom_vecs_[layer_id];
@@ -636,6 +649,7 @@
         << " diff: " << diff_abs_val_mean;
   }
 }
+#endif
 
 template <typename Dtype>
 void Net<Dtype>::UpdateDebugInfo(const int param_id) {
@@ -651,6 +665,7 @@
         << ", param " << param_display_name
         << " data: " << data_abs_val_mean
         << "; diff: " << diff_abs_val_mean;
+#
   } else {
     const string& owner_layer_name =
         layer_names_[param_layer_indices_[param_owner].first];
@@ -695,6 +710,7 @@
   }
 }
 
+#ifdef ENABLE_BACKWARD
 template <typename Dtype>
 void Net<Dtype>::BackwardFrom(int start) {
   BackwardFromTo(start, 0);
@@ -723,6 +739,7 @@
                << "L2 norm = (" << l2norm_data << ", " << l2norm_diff << ")";
   }
 }
+#endif
 
 template <typename Dtype>
 void Net<Dtype>::Reshape() {
@@ -771,10 +788,14 @@
 
 template <typename Dtype>
 void Net<Dtype>::CopyTrainedLayersFrom(const string trained_filename) {
+#ifdef USE_HDF5
   if (trained_filename.size() >= 3 &&
       trained_filename.compare(trained_filename.size() - 3, 3, ".h5") == 0) {
     CopyTrainedLayersFromHDF5(trained_filename);
   } else {
+#else
+  {
+#endif
     CopyTrainedLayersFromBinaryProto(trained_filename);
   }
 }
@@ -787,8 +808,10 @@
   CopyTrainedLayersFrom(param);
 }
 
+
 template <typename Dtype>
 void Net<Dtype>::CopyTrainedLayersFromHDF5(const string trained_filename) {
+#ifdef USE_HDF5
   hid_t file_hid = H5Fopen(trained_filename.c_str(), H5F_ACC_RDONLY,
                            H5P_DEFAULT);
   CHECK_GE(file_hid, 0) << "Couldn't open " << trained_filename;
@@ -835,6 +858,7 @@
   }
   H5Gclose(data_hid);
   H5Fclose(file_hid);
+#endif
 }
 
 template <typename Dtype>
@@ -851,6 +875,7 @@
 
 template <typename Dtype>
 void Net<Dtype>::ToHDF5(const string& filename, bool write_diff) const {
+#ifdef USE_HDF5
   hid_t file_hid = H5Fcreate(filename.c_str(), H5F_ACC_TRUNC, H5P_DEFAULT,
       H5P_DEFAULT);
   CHECK_GE(file_hid, 0)
@@ -904,14 +929,17 @@
     H5Gclose(diff_hid);
   }
   H5Fclose(file_hid);
+#endif
 }
 
+#ifdef ENABLE_BACKWARD
 template <typename Dtype>
 void Net<Dtype>::Update() {
   for (int i = 0; i < learnable_params_.size(); ++i) {
     learnable_params_[i]->Update();
   }
 }
+#endif
 
 template <typename Dtype>
 void Net<Dtype>::ClearParamDiffs() {
